---
{"dg-publish":true,"permalink":"//entropy/"}
---

해당 분포 [[정보 이론/정보량\|정보량]]의 기대값으로,

$$ \mathbf{H(p) = - \sum_{x} p(x) \log p(x)} = E{x∼p}[−log⁡p(x)]$$
# 직관
풀어서 설명하면, 랜덤 변수 XX가 분포 pp를 따른다고 할 때, 사건 xx가 일어났을 때의 “놀라움”을 −log⁡p(x) ([[정보 이론/정보량\|정보량]]), H(p)는 이 놀라움의 **기댓값**.
압축을 완전히 잘했을 때, 한 심볼을 나타내는 데 드는 평균 비트 수의 이론적 하한