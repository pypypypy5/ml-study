---
{"dg-publish":true,"permalink":"/ml/gen-model/auto-encoder/"}
---

차원을 줄이고, 늘이는 것은 고차원 벡터에서 중요한 정보를 추출하여 저차원으로 압축하는 것.

→ 그렇다면 일부러 차원을 확 줄이고, 다시 늘이고, 정답으로는 원래 입력을 줘서, 그 “병목” 레이어만의 정보로 원래 입력을 복원할 수 있도록 학습시키면 데이터의 압축 - 복원을 신경망에 학습 시킬 수 있을 것.

Denoising autoencoder: 아예 입력에 노이즈를 주고 노이즈 없는 입력을 복원하도록 시켜서 압축 능력을 향상시키자.

이런 식으로 추가 제약 줘서 복원 능력 키우는 시도들도 많음

+) [[확률-통계/MLE\|MLE]]를 사용한다. 여기서 posterior는 p_θ(z∣x)가 된다.

auto encoder의 문제는,

1. [[likelihood\|likelihood]]를 계산 불가능.

극대화해야 하는 likelihood는 다음과 같이 정의된다.

$$ p_θ(x)=∫p(z)p_θ(x∣z)dz $$

그런데 z가 고차원이라 계산 어렵고, p_θ(x∣z)는 neural net이라 절대 analytic하게 적분이 불가능하다.

→ likelihood를 계산할 수 없다.

1. latent space가 데이터 주변에 쏠려 있다.

본래라면 latent space가 전체 공간 내에 골고루 퍼져 있어야 하지만 auto encoder는 공간이 데이터 주변에 쏠려 있다. 오른쪽 아래에 “개” 점들, 중간에 직선형으로 “인간” 점들이 분포하는 식으로 일부에만 데이터가 쏠려 있고, 나머지 대부분의 공간은 아무런 쓸모도 가지지 못한다.

→ 나중에 latent space에서 샘플링, decode 할때 빈 공간에서 추출하면 의미를 가지지 못하는 점이라 이상한 노이즈, 뒤틀림 등이 나온다.