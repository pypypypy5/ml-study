---
{"dg-publish":true,"permalink":"/ml/rl/model-based-rl/"}
---

# 정의
여기서 model이란 policy나 vlaue function에 해당하는 mlp 따위가 아니라 실제 env를 재현한 것. 디지털 트윈 같은거라 보면 된다.

여기서 실제로 에이전트가 action, reward를 받으며 실제 환경 대신에 가상현실 안에서 학습하는 것이다.

## 방법론

## [[mpc\|mpc]]

policy, value function을 훈련시키지 않고 inference 시에도 모델 내에서 시뮬레이션을 그때그때 돌리며 가장 보상이 큰 행동을 직접 찾는다.

**문제**

오차가 누적되며 모델의 환각이 심해진다. 잘못된 길을 가는 것.

## [[grad based\|grad based]]

문제

우연하게 성공할때마다 보상을 받을 뿐, 정확히 어떻게 성공 실패했는지는 환경의 원리가 블랙박스라 모른다.

해결

state도 모델로 만들어 통제 가능하니, 아예 역전파를 할때 모델 부분도 포함시키자.

보상 → 모델 (상태) → policy 식으로 역전파가 전파되도록 하는 것.

→ 환경을 미분 가능한 수식으로 만들고 역전파에 넣음으로써 이 행동이 어떻게 성공했고, 행동을 정량적으로 어떻게 바꾸면 어떻게 보상이 변화할지의 정보를 얻을 수 있다.

**문제**

층이 깊어지며 기울기 폭발, 소실의 문제 발생

## [[ML/RL/model-free rl\|model-free rl]]

앞의 model-based의 문제를 해결하고자 모델은 데이터 생성기로만 사용하는 방식들.