---
{"dg-publish":true,"permalink":"/ml/bnn/"}
---

# 문제
기존 [[확률-통계/MLE\|MLE]] 방식은 확률 분포를 일차원적으로 단순 모델링.
-> 모르는 것을 모른다고 표현할 수 없다.
ex. 개, 고양이 구별하는 모델에 오리를 넣으면 개 80%, 고양이 20%
# 해결
mle가 아닌 [[확률-통계/Bayesian\|Bayesian]]을 사용하여 학습된 분포 밖의 데이터를 마주하면 자각할 수 있도록 한다.
## 기본 개념
각 파라미터를 하나의 고정된 숫자가 아닌 평균과 분산 둘을 가진 확률분포로 생각.
-> mle는 데이터 사이를 구분하는 하나의 직선을 가지지만, bnn은 직선이 아닌 직선 주변의 확률 분포 - 구름을 학습한다.
데이터 D가 주어졌을때 가장 적절한 가중치 w의 사후 분포를 학습한다.
$$P(w|D) = \frac{P(D|w)P(w)}{P(D)}$$
그리고 추론은 고정된 가중치로 한번 돌리는 것이 아니라 각 w에서 샘플링한 여러 가중치들로 여러번 추론 돌린다. 결과는 여러 결과의 평균으로 내고, 결과들의 분산이 크면 "학습 데이터 바깥의 분포, **모델이 모르는 케이스**"임을 알 수 있다.
-> 직관적으로 데이터가 많은 곳이면 선이 촘촘, 데이터 없는 - 모델이 모르는 - 곳이면 선이 듬성듬성
## 학습
위의 식에서 $P(D)=\int P(D|w)P(w)dw$인데 이건 적분이 매우 어렵다. 그래서 [[ML/Variational Inference\|Variational Inference]]를 사용해서 모델의 분포가 실제 분포와 가까워지도록 학습. 
그런데 w의 샘플링은 미분 불가능하니 대신 w의 분포에 대해 [[ML/general/reparametrization trick\|reparametrization trick]]을 사용한다.