---
{"dg-publish":true,"permalink":"/ml/general/lo-ra/"}
---

일반 학습은 해당 노드의 가중치 W (4096*4096) 전체를 업데이트. 엄청난 연산 자원이 든다.

그걸 해결하기 위해 W→ W+W_delta 대신 W+AB→W+AB+AB_delta로 업데이트 하는 방식.

A (4096_8), B(8_4096)

변화는 AB안에서만 일어나고 원래 가중치 W는 그대로 유지된다. 그러니 full training과 거의 같은 효과를 내면서 변화하는 값, 연산은 훨씬 적다, 싸다.

A, B를 기저 행렬로 압축하여 중요하지 않은 정보를 제거하고 정보를 압축하는 것. 근본적으로 SVD, PCA와 유사한 것이라 한다.

다만 정보의 손실이 존재해 모델에 큰 역할 차이를 주기 위해 훈련하면 제대로 적용되지 않을 수 있다고 한다. 반면 본 행렬이 변하지 않으니 역으로 훈련 중 중요한 정보가 날아가는 catastrophic forgetting을 막을 수 있다고.

원리는 svd, pca와 유사하니 나중에 공부하고 돌아와서 다시 봐보자.